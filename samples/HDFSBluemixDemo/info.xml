<?xml version="1.0" encoding="UTF-8"?>
<info:toolkitInfoModel xmlns:common="http://www.ibm.com/xmlns/prod/streams/spl/common" xmlns:info="http://www.ibm.com/xmlns/prod/streams/spl/toolkitInfo">
  <info:identity>
    <info:name>TestForBluemix</info:name>
    <info:description>This sample contains applications that can run within the StreamingAnalytics service and connect to a Hadoop instance also running on Bluemix.
There are 3 composites in this project:
* hdfsexample::TestWrite
* hdfsexample::TestRead
* hdfsexample::TestDirScan.
Each composite tests one of the HDFS* operators: HDFS2FileSink, HDFS2FileSource, and HDFS2DirectoryScan respectively. 
It is suggested to run the `hdfsexample::TestWrite` composite first in order for the remaining applications to run successfully.


# Compiling the samples
Compiling in Streams Studio:
1. Start Streams Studio
2. In Streams Explorer view, add the &quot;com.ibm.streamsx.hdfs&quot; toolkit as one of the toolkit locations
3. From the main menu, select File -> Import.
4. Select General ->Existing Project into Workspace
5. Browse to the location of the HDFSBluemixDemo project and select it
6. Once the sample is imported, wait for the build to finish.  If autobuild is turned off, select resulting project, right click -> Build Project



Compiling at the command line:
1. Create a directory. For example, you can create a directory in your home directory. 
2. Copy the `samples` folder to this directory.
3. Build the sample applications. Go to the `HDFSBluemixDemo` subdirectory and run `make`. By default, the sample is compiled as a distributed application. If you want to compile the application as a standalone application, run `make standalone` instead. Run `make clean` to revert the samples back to their original uncompiled state.



# Running the samples from Streams Studio.
Follow the instructions to compile above.  Once the project is built, select the main composite of the sample, right click -> Launch Active Build Config 

# Running using the Streaming Analytics service:
 * Go to the Application Dashboard in your browser and click &quot;submit job&quot;.  
 * Browse to the output/Distributed folder and select the .sab file for the application you wish to launch.
 * Specify the uri, username and password as submission parameters and submit the job.

# Running from the command line:
The following are examples of how to launch the `TestDirScan` sample which demonstrates the use of the `HDFS2DirectoryScan` operator:
* Running in standalone mode: 
  * From the samples directory, run the following command:
  
   `./output/Standalone/hdfsexample::&lt;TestDirScan>/bin/standalone hdfsUri=&quot;webhdfs://&lt;machine_name>:&lt;port>&quot; hdfsUser=&quot;&lt;user_name>&quot; hdfsPassword=&quot;&lt;password>&quot; -P directory="/tmp"` 
* Running in distributed mode:  
  * If using a local installation of Streams, start your IBM InfoSphere Streams instance, then use the `streamtool` command to submit the .adl files that were generated during the application build: 

   `streamtool submitjob -d &lt;domain_name> -i &lt;instance_name> output/Distributed/hdfsexample::TestDirScan/hdfsexample.TestDirScan.adl -P hdfsUri=&quot;webhdfs://&lt;machine_name>:&lt;port>&quot; -P hdfsUser=&quot;&lt;user_name>&quot; -P directory="/tmp" -P hdfsPassword=&quot;&lt;password>&quot;`
</info:description>
    <info:version>1.0.0</info:version>
    <info:requiredProductVersion>4.1.0.0</info:requiredProductVersion>
  </info:identity>
  <info:dependencies>
    <info:toolkit>
      <common:name>com.ibm.streamsx.hdfs</common:name>
      <common:version>[3.5.0,4.0.0)</common:version>
    </info:toolkit>
  </info:dependencies>
</info:toolkitInfoModel>
