/*******************************************************************************
* Copyright (C) 2014, International Business Machines Corporation
* All Rights Reserved
*******************************************************************************/                      
namespace hdfsexample ;

use com.ibm.streamsx.hdfs::HDFS2DirectoryScan ;
use com.ibm.streamsx.hdfs::HDFS2FileSource ;

/**
 * The [TestDirScan] composite demonstrates how you can use the HDFS2DirectoryScan operator
 * to scan for new files from a directory in HDFS on Bluemix.
  The HDFS2DirectoryScan can be used in conjunction
 * with the HDFS2FileSource operator, where you can read new files from HDFS as they are created in 
 * a directory.
 * 
 * To see the effect of HDFS2DirectoryScan, let the application run for a few minutes, and then copy more
 * files to the testDirectory.  You should see that the directory scan will discover the new files for the
 * HDFS2FileSource operator to read.
 *
 * To build this application, you need to set up the HADOOP_HOME environment variable as follows:
 * * export HADOOP_HOME=<Hadoop installation>
 * 
 * Setup:
 * You will need to create new directory for the HDFS2DirectoryScan operator to scan.
 * 1. Create a new directory in HDFS by running this command at the command line, if you have SSH access:
 *  * <HADOOP_HOME>/bin/hadoop fs -mkdir /user/<userid>/testDirectory
 * 2. Copy a few files from local file system to the testDirectory on HDFS:
 *  * <HADOOP_HOME>/bin/hadoop fs -copyFromLocal <localFile> /user/<userid>/testDirectory
 * Otherwise, you could test it by scanning the /tmp directory of Hadoop filesystem or the /user/<userid> directory, where <userid> is teh same id used to authenticate.
 * To compile and run this sample in Streams Studio:
 * 1. Start Streams Studio
 * 2. In Streams Explorer view, add the "com.ibm.streams.bigdata" toolkit as one of the toolkit locations
 * 3. From the main menu, select File -> Import.
 * 4. Expand InfoSphere Streams Studio
 * 5. Select Sample SPL Application
 * 6. Select this sample from the Import SPL Sample Application dialog
 * 7. Once the sample is imported, wait for the build to finish.  If autobuild is turned off, select resulting project, right click -> Build Project
 * 8. Once the project is built, select the main composite of the sample, right click -> Launch Active Build Config * 
 *
 * To compile and run this sample at the command line:
 *  1. Create a directory. For example, you can create a directory in your home directory. 
 *   * mkdir $HOME/hdfssamples
 *  2. Copy the samples to this directory. For example, you can copy the samples to the directory created above. 
 *   * cp -R <path to hdfs bluemix toolkit>/samples/* $HOME/hdfssamples/
 *  3. Build the sample applications. Go to the HDFSBluemixDemo subdirectory and run the make. By default, the sample is compiled as a distributed application. If you want to compile the application as a standalone application, run make standalone instead. Run make clean to return the samples back to their original state.
 *  4. Run the sample application. 
 *   * To run the TestDirScan sample application in distributed mode:
 * 		* If using the Streaming Analytics service, go to the Application Dashboard in your browser and click "submit job".  Browse to the output/Distributed folder and upload the .sab file to the service.
 * 	    * If using a local installation of Streams, start your IBM InfoSphere Streams instance, then use the streamtool command to submit the .adl files that were generated during the application build. 
 *    		* streamtool submitjob -i <instance_name> output/Distributed/hdfsexample::TestDirScan/hdfsexample.TestDirScan.adl -P hdfsUri="webhdfs://<machine_name>:<port>" -P hdfsUser="<user_name>" -P hdfsPassword="<password>"
 *   * To run the TestDirScan sample application in standalone mode, issue the command:
 *    * ./output/Standalone/hdfsexample::TestDirScan/bin/standalone hdfsUri="webhdfs://<machine_name>:<port>" hdfsUser="<user_name>" hdfsPassword="<password>"
 *
 * @param hdfsUri URI to HDFS. 
 * @param hdfsUser User to connect to HDFS.
 * @param hdfsPassword Password to connect to HDFS.
 * 
 *  
 */
composite TestDirScan
{
	param
		expression<rstring> $user : getSubmissionTimeValue("hdfsUser");
		expression<rstring> $password : getSubmissionTimeValue("hdfsPassword");
		expression<rstring> $uri :getSubmissionTimeValue("hdfsUri"); //format webhdfs://host:port
		
	graph

		// scan the given directory from HDFS, default to . which is the user's home directory
		stream<rstring fileNames> FileNameStream1 = HDFS2DirectoryScan()
		{
			param
				directory : getSubmissionTimeValue("directory", ".");
				hdfsUri : $uri ;
				hdfsUser:$user;
				hdfsPassword: $password;
		}

		// use the file name from directory scan to read the file
		stream<rstring lines> LineStream = HDFS2FileSource(FileNameStream1)
		{
			param
				hdfsUri : $uri ;
				hdfsUser:$user;
				hdfsPassword: $password;
		} 

		//write out content of file in the "FileContents.txt" file in data directory
		() as NameSink1 = Custom(FileNameStream1)
		{
			logic
				onTuple FileNameStream1: {
					printStringLn("Found file in directory: " + fileNames);
				}
		}
		// write out content of file in the "FileContents.txt" file in data directory
		() as Lines = FileSink(LineStream)
		{
			param
				file: "/tmp/FileContents.txt";
				flush: 1u;
		}

}
